{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "b9b8b23f-7d7d-404e-889d-8cc9ff52b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have downloaded the dataset from https://www.kaggle.com/rounakbanik/the-movies-dataset and stored in archive folder\n",
    "# Load the data using pandas\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "538aa13d-592e-424d-89e2-2e2744e39be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie metadata \n",
    "df_data = pd.read_csv('archive/movies_metadata.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "2f7b4b6b-812b-49fb-bcd8-313f610be640",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords = pd.read_csv('archive/keywords.csv')\n",
    "\n",
    "# Some ids have irregular format, so we will remove them\n",
    "df_cb = df_data.copy(deep=True)[df_data.id.apply(lambda x: x.isnumeric())]\n",
    "df_cb['id'] = df_cb['id'].astype(int)\n",
    "df_keywords['id'] = df_keywords['id'].astype(int)\n",
    "\n",
    "# Merging keywords, credits of movies with main data set\n",
    "df_movies_data = pd.merge(df_cb, df_keywords, on='id')\n",
    "df_movies_data = df_movies_data.drop(['status','tagline','vote_average','vote_count','keywords', 'budget', 'adult', 'original_language','production_companies','production_countries', 'revenue','runtime','spoken_languages','belongs_to_collection', 'homepage', 'imdb_id', 'original_title', 'overview', 'poster_path', 'release_date', 'video'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "1e5cc92b-d820-4e98-808f-5966566711e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         {'colors': ['Animation', 'Comedy', 'Family']}\n",
      "1        {'colors': ['Adventure', 'Fantasy', 'Family']}\n",
      "2                     {'colors': ['Romance', 'Comedy']}\n",
      "3            {'colors': ['Comedy', 'Drama', 'Romance']}\n",
      "4                                {'colors': ['Comedy']}\n",
      "                              ...                      \n",
      "46477                   {'colors': ['Drama', 'Family']}\n",
      "46478                             {'colors': ['Drama']}\n",
      "46479       {'colors': ['Action', 'Drama', 'Thriller']}\n",
      "46480                                    {'colors': []}\n",
      "46481                                    {'colors': []}\n",
      "Name: genres, Length: 46482, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#We are processing the genres column to do the hot encoding, This could be done better, at this time, I'm trying this approach.\n",
    "import json\n",
    "\n",
    "data = {'color': [[{'name':'red'},{'name':'blue'}], [{'name':'blue'}], [{'name':'green'}], [{'name':'red'}], [{'name':'blue'}]]}\n",
    "df = pd.DataFrame(data)\n",
    "cat = []\n",
    "def list_me(df):\n",
    "    a = []\n",
    "    dict = {}\n",
    "    X = df['genres']\n",
    "    # Replace single quotes with double quotes\n",
    "    X = str(X).replace(\"'\", '\"')\n",
    "    \n",
    "    #convert string to  object\n",
    "    X = json.loads(X)\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        a.append(x['name'])\n",
    "        if x['name'] not in cat: \n",
    "            cat.append(x['name'])\n",
    "    dict = {}\n",
    "    dict[\"colors\"] = a\n",
    "    #return json.dumps(dict, indent = 4)\n",
    "    return dict\n",
    "\n",
    "df_movies_data['genres'] = df_movies_data.apply(list_me, axis = 1)\n",
    "print(df_movies_data['genres'])\n",
    "temp = pd.get_dummies(pd.DataFrame.from_records(df_movies_data.genres.values)['colors'].explode())\n",
    "temp = temp.replace({True: 1, False: 0})\n",
    "temp = temp.groupby(temp.index).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "710feea6-c6f8-4553-bd47-3dae8d6a215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index      id popularity                             title  Action  \\\n",
      "0       4356       2   3.860491                             Ariel       0   \n",
      "1      12991       3    2.29211               Shadows in Paradise       0   \n",
      "2         17       5   9.026586                        Four Rooms       0   \n",
      "3        474       6   5.538671                    Judgment Night       1   \n",
      "4        256      11  42.149697                         Star Wars       1   \n",
      "...      ...     ...        ...                               ...     ...   \n",
      "46477  46095  465044   0.281008                         Abduction       0   \n",
      "46478  46289  467731   0.001189       Tragedy in a Temporary Town       0   \n",
      "46479  21965  468343   0.001202          Silja - nuorena nukkunut       0   \n",
      "46480  46414  468707   0.347806  Thick Lashes of Lauri MÃ¤ntyvaara       0   \n",
      "46481  20268  469172   0.001097   Manuel on the Island of Wonders       0   \n",
      "\n",
      "       Adventure  Animation  Comedy  Crime  Documentary  ...  History  Horror  \\\n",
      "0              0          0       0      1            0  ...        0       0   \n",
      "1              0          0       1      0            0  ...        0       0   \n",
      "2              0          0       1      1            0  ...        0       0   \n",
      "3              0          0       0      1            0  ...        0       0   \n",
      "4              1          0       0      0            0  ...        0       0   \n",
      "...          ...        ...     ...    ...          ...  ...      ...     ...   \n",
      "46477          0          0       0      0            0  ...        0       0   \n",
      "46478          0          0       0      0            0  ...        0       0   \n",
      "46479          0          0       0      0            0  ...        0       0   \n",
      "46480          0          0       1      0            0  ...        0       0   \n",
      "46481          0          0       0      0            0  ...        0       0   \n",
      "\n",
      "       Music  Mystery  Romance  Science Fiction  TV Movie  Thriller  War  \\\n",
      "0          0        0        0                0         0         0    0   \n",
      "1          0        0        0                0         0         0    0   \n",
      "2          0        0        0                0         0         0    0   \n",
      "3          0        0        0                0         0         1    0   \n",
      "4          0        0        0                1         0         0    0   \n",
      "...      ...      ...      ...              ...       ...       ...  ...   \n",
      "46477      0        0        0                0         0         0    0   \n",
      "46478      0        0        0                0         0         0    0   \n",
      "46479      0        0        1                0         0         0    0   \n",
      "46480      0        0        1                0         0         0    0   \n",
      "46481      0        0        0                0         0         0    0   \n",
      "\n",
      "       Western  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "...        ...  \n",
      "46477        0  \n",
      "46478        0  \n",
      "46479        0  \n",
      "46480        0  \n",
      "46481        0  \n",
      "\n",
      "[46482 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df_movies_data = df_movies_data.drop(columns=['genres'])\n",
    "df_movies_data = df_movies_data.join(temp, how=\"inner\")\n",
    "df_movies_data = df_movies_data.sort_values(by=['id']).reset_index()\n",
    "df_x1 = df_movies_data.drop(columns=['index','id','title','popularity'])\n",
    "print(df_movies_data)\n",
    "# Now we have the movie data with some features. This is our X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "2b28d2ca-a071-403f-a740-6d5bfb5cadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the dataset , we have only user id as the user feature.Now, we need to generate some features from the data. For instance, avaerage rating per genre for each user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "a65a6bfd-6f7a-48f9-bb1a-fe4f7b59320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv('archive/ratings_small.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "a7f14454-d37e-4838-8801-cc8bf346c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing reduce()\n",
    "from functools import reduce\n",
    " \n",
    "def average(lst):\n",
    "    return reduce(lambda a, b: a + b, lst) / len(lst)\n",
    "\n",
    "\n",
    "ratings = df_ratings.pivot(index='userId', columns='movieId', values='rating')\n",
    "ratings = ratings.fillna(0)\n",
    "ratings_another = ratings.copy()\n",
    "movieIds = []\n",
    "for col in ratings.columns:\n",
    "    movieIds.append(col)\n",
    "genres_name= []\n",
    "for col in df_movies_data.columns:\n",
    "    genres_name.append(col)\n",
    "genres_name = genres_name[4:]\n",
    "\n",
    "# Initialize data of lists\n",
    "#data = [{'b': 2, 'c': 3}, {'a': 10, 'b': 20, 'c': 30}]\n",
    "data = {}\n",
    "  \n",
    "# Creates pandas DataFrame by passing\n",
    "# Lists of dictionaries and row index.\n",
    "#df = pd.DataFrame(data, index=['first', 'second'])\n",
    "df_x2 = pd.DataFrame(data, index=[])\n",
    "\n",
    "# Iterate all rows using DataFrame.iterrows()\n",
    "for user, row in ratings.iterrows():\n",
    "    \n",
    "    # All the movie rated by the user\n",
    "    row = row.to_numpy()\n",
    "\n",
    "    # Get the column index of  movies with a rating greater than 0.0. The column index is the id of the film.\n",
    "    r_movies = np.where(row > 0.0)\n",
    "\n",
    "    # Converted to a list\n",
    "    rated_movies = r_movies[0].tolist()\n",
    "    user_ratings = {}\n",
    "\n",
    "    #iterate through ids\n",
    "    for i in range(len(rated_movies)):\n",
    "        \n",
    "        # One movie at a time rate by the user j\n",
    "        movie_df = df_movies_data.loc[df_movies_data['id'] == rated_movies[i]] \n",
    "        score = row[rated_movies[i]]\n",
    "        if movie_df.shape[0] == 1:\n",
    "            \n",
    "            # Convert the movie data to a list\n",
    "            movie = movie_df.values[0].tolist()\n",
    "            \n",
    "            # Get the genres-only movie data\n",
    "            genres_all = np.array(movie[4:])\n",
    "\n",
    "            # Get the genres that are being activated for this particular movie.\n",
    "            genres = np.where(genres_all == 1)[0]\n",
    "\n",
    "            # List through genres\n",
    "            for k in range(len(genres.tolist())):\n",
    "                genre = genres_name[genres.tolist()[k]]\n",
    "                if genre in user_ratings.keys():\n",
    "                    user_ratings[genre].append(score)\n",
    "                else:\n",
    "                    user_ratings[genre] = []\n",
    "                    user_ratings[genre].append(score)\n",
    "    #break\n",
    "\n",
    "    for key in user_ratings:\n",
    "        user_ratings[key] = round(average(user_ratings[key]),2)\n",
    "\n",
    "    index = []\n",
    "    index.append(user)\n",
    "    df_x2 = pd.concat([df_x2, pd.DataFrame(user_ratings, index=index)])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "da1cddbd-fbe6-488c-b964-ffacf2498c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Animation  Science Fiction  Drama  Romance  Horror  Thriller  Action  \\\n",
      "1         2.50             2.50   3.20     3.00    4.00      3.00    2.00   \n",
      "2         4.00             3.00   3.63     3.70    4.00      3.55    3.31   \n",
      "3         0.00             0.00   3.73     4.00    3.50      3.61    3.58   \n",
      "4         5.00             4.56   4.52     4.50    4.30      4.50    4.12   \n",
      "5         4.00             4.00   3.76     3.92    4.00      4.11    3.95   \n",
      "..         ...              ...    ...      ...     ...       ...     ...   \n",
      "667       4.50             3.50   3.59     3.69    4.33      4.00    3.67   \n",
      "668       0.00             3.00   3.67     3.75    0.00      3.50    4.00   \n",
      "669       0.00             4.00   3.12     4.33    4.00      2.50    4.00   \n",
      "670       3.50             4.50   3.67     4.40    2.50      4.50    5.00   \n",
      "671       4.75             3.75   3.95     4.05    4.25      3.88    4.03   \n",
      "\n",
      "     Crime  Mystery  Comedy  Music  Adventure  Fantasy  Family  Documentary  \\\n",
      "1     2.00     2.00    3.00    0.0       0.00     0.00    0.00         0.00   \n",
      "2     3.76     3.60    3.50    4.0       3.10     2.86    3.00         4.00   \n",
      "3     3.50     3.25    3.81    3.0       4.00     0.00    0.00         0.00   \n",
      "4     4.52     4.62    4.32    3.5       4.44     4.70    4.60         3.75   \n",
      "5     3.94     3.62    3.88    5.0       3.94     3.83    3.17         4.50   \n",
      "..     ...      ...     ...    ...        ...      ...     ...          ...   \n",
      "667   3.42     3.40    3.62    3.5       4.00     4.17    4.00         0.00   \n",
      "668   5.00     0.00    4.00    5.0       4.00     0.00    0.00         0.00   \n",
      "669   3.67     3.00    3.00    3.0       4.00     3.00    3.50         0.00   \n",
      "670   4.50     4.00    4.22    5.0       4.00     3.50    3.50         0.00   \n",
      "671   4.00     3.83    3.87    4.0       4.19     4.25    4.75         0.00   \n",
      "\n",
      "      War  History  Western  Foreign  TV Movie  \n",
      "1    0.00     0.00     0.00     0.00       0.0  \n",
      "2    3.00     4.50     3.00     0.00       0.0  \n",
      "3    0.00     4.00     3.50     0.00       0.0  \n",
      "4    4.33     4.33     3.33     4.67       0.0  \n",
      "5    3.75     4.00     3.50     0.00       5.0  \n",
      "..    ...      ...      ...      ...       ...  \n",
      "667  3.00     4.00     0.00     0.00       0.0  \n",
      "668  0.00     0.00     0.00     0.00       0.0  \n",
      "669  0.00     0.00     3.00     0.00       0.0  \n",
      "670  0.00     0.00     0.00     2.00       0.0  \n",
      "671  0.00     4.33     0.00     0.00       4.5  \n",
      "\n",
      "[671 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "df_x2 = df_x2.fillna(0)\n",
    "print(df_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "949741f1-5c83-49f4-9934-e85895ef3ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " sequential_26 (Sequential)     (None, 32)           42400       ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " sequential_27 (Sequential)     (None, 32)           42400       ['input_26[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_24 (TFOpL  (None, 32)          0           ['sequential_26[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_25 (TFOpL  (None, 32)          0           ['sequential_27[0][0]']          \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " dot_12 (Dot)                   (None, 1)            0           ['tf.math.l2_normalize_24[0][0]',\n",
      "                                                                  'tf.math.l2_normalize_25[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 84,800\n",
      "Trainable params: 84,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the NN arcitecture\n",
    "num_outputs = 32\n",
    "tf.random.set_seed(1)\n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs, activation='linear')\n",
    "])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs, activation='linear')\n",
    "])\n",
    "\n",
    "num_user_features = len(df_x2.columns)\n",
    "num_item_features = len(df_x1.columns)\n",
    "\n",
    "# Create the user input and point to the base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
    "\n",
    "# Create the item input and point to the base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
    "\n",
    "# Compute the dot product of the two vectors vu and vm\n",
    "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
    "\n",
    "# Specify the inputs and output of the model\n",
    "model = tf.keras.Model([input_user, input_item], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "5da02598-3a78-4a37-ad12-033490c29cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6877, 20)\n",
      "(6877, 20)\n",
      "(6877,)\n"
     ]
    }
   ],
   "source": [
    "# Setup the training data such a way that input to the NNs are a user and his/her movies. The plan is to create same number of movies and user data. So, we need to duplicate the user data as user has rated multiple movies.\n",
    "# For instance,\n",
    "# movie 1 - user 1\n",
    "# movie2 - user 2\n",
    "# movie5 - user 3\n",
    "#\n",
    "item_list = []\n",
    "user_list = []\n",
    "y = []\n",
    "\n",
    "# Iterate all rows using DataFrame.iterrows()\n",
    "for user, row in ratings_another.iterrows():\n",
    "    \n",
    "    # All the movie rated by the user\n",
    "    row = row.to_numpy()\n",
    "\n",
    "    # Get the column index of  movies with a rating greater than 0.0. The column index is the id of the film.\n",
    "    movies = np.where(row > 0.0)\n",
    "    movies = movies[0].tolist()\n",
    "    for movie in movies:\n",
    "        movie_df = df_movies_data.loc[df_movies_data['id'] == movie] \n",
    "        movie_df = movie_df.drop(columns=['index', 'id', 'title','popularity'])\n",
    "        if np.any(movie_df.to_numpy()):\n",
    "            item_list.append(movie_df.to_numpy())\n",
    "            user_list.append(df_x2.loc[[user]].to_numpy())\n",
    "            y.append(row[movie])\n",
    "\n",
    "    if user == 100:\n",
    "        break\n",
    "\n",
    "item_train = np.vstack(item_list)\n",
    "user_train = np.vstack(user_list)\n",
    "item_train = item_train[:user_train.shape[0]:]\n",
    "y = np.array(y)\n",
    "print(item_train.shape)\n",
    "print(user_train.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "d20d06f7-6ac1-4b9f-a34c-8289fd38e3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "a1e8ceec-ca05-4c95-a9be-316143b2ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "item_train_unscaled = item_train\n",
    "user_train_unscaled = user_train\n",
    "y_train_unscaled    = y\n",
    "\n",
    "scalerItem = StandardScaler()\n",
    "scalerItem.fit(item_train)\n",
    "item_train = scalerItem.transform(item_train)\n",
    "\n",
    "scalerUser = StandardScaler()\n",
    "scalerUser.fit(user_train)\n",
    "user_train = scalerUser.transform(user_train)\n",
    "\n",
    "scalerTarget = MinMaxScaler((-1, 1))\n",
    "scalerTarget.fit(y.reshape(-1, 1))\n",
    "y_train = scalerTarget.transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "7f410e0c-063e-4b3b-b7e0-5032c11e92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train-test set\n",
    "item_train, item_test = train_test_split(item_train, train_size=0.80, random_state=1)\n",
    "user_train, user_test = train_test_split(user_train, train_size=0.80, random_state=1)\n",
    "y_train, y_test       = train_test_split(y_train,    train_size=0.80, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "52249f93-39ca-40ec-b754-a4ec7c849631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.50263423 -0.42282333 -0.16764798 ... -0.55579448 -0.18768337\n",
      "  -0.15872554]\n",
      " [-0.50263423 -0.42282333 -0.16764798 ... -0.55579448 -0.18768337\n",
      "  -0.15872554]\n",
      " [-0.50263423 -0.42282333 -0.16764798 ... -0.55579448 -0.18768337\n",
      "  -0.15872554]\n",
      " ...\n",
      " [-0.50263423 -0.42282333 -0.16764798 ... -0.55579448 -0.18768337\n",
      "  -0.15872554]\n",
      " [-0.50263423 -0.42282333 -0.16764798 ... -0.55579448 -0.18768337\n",
      "  -0.15872554]\n",
      " [-0.50263423 -0.42282333 -0.16764798 ... -0.55579448 -0.18768337\n",
      "  -0.15872554]]\n",
      "Epoch 1/60\n",
      "172/172 [==============================] - 1s 735us/step - loss: 0.2038\n",
      "Epoch 2/60\n",
      "172/172 [==============================] - 0s 657us/step - loss: 0.1964\n",
      "Epoch 3/60\n",
      "172/172 [==============================] - 0s 660us/step - loss: 0.1950\n",
      "Epoch 4/60\n",
      "172/172 [==============================] - 0s 651us/step - loss: 0.1935\n",
      "Epoch 5/60\n",
      "172/172 [==============================] - 0s 663us/step - loss: 0.1934\n",
      "Epoch 6/60\n",
      "172/172 [==============================] - 0s 656us/step - loss: 0.1915\n",
      "Epoch 7/60\n",
      "172/172 [==============================] - 0s 647us/step - loss: 0.1918\n",
      "Epoch 8/60\n",
      "172/172 [==============================] - 0s 653us/step - loss: 0.1912\n",
      "Epoch 9/60\n",
      "172/172 [==============================] - 0s 660us/step - loss: 0.1895\n",
      "Epoch 10/60\n",
      "172/172 [==============================] - 0s 675us/step - loss: 0.1891\n",
      "Epoch 11/60\n",
      "172/172 [==============================] - 0s 674us/step - loss: 0.1893\n",
      "Epoch 12/60\n",
      "172/172 [==============================] - 0s 658us/step - loss: 0.1892\n",
      "Epoch 13/60\n",
      "172/172 [==============================] - 0s 653us/step - loss: 0.1888\n",
      "Epoch 14/60\n",
      "172/172 [==============================] - 0s 654us/step - loss: 0.1875\n",
      "Epoch 15/60\n",
      "172/172 [==============================] - 0s 650us/step - loss: 0.1866\n",
      "Epoch 16/60\n",
      "172/172 [==============================] - 0s 662us/step - loss: 0.1869\n",
      "Epoch 17/60\n",
      "172/172 [==============================] - 0s 653us/step - loss: 0.1853\n",
      "Epoch 18/60\n",
      "172/172 [==============================] - 0s 655us/step - loss: 0.1851\n",
      "Epoch 19/60\n",
      "172/172 [==============================] - 0s 666us/step - loss: 0.1853\n",
      "Epoch 20/60\n",
      "172/172 [==============================] - 0s 677us/step - loss: 0.1841\n",
      "Epoch 21/60\n",
      "172/172 [==============================] - 0s 671us/step - loss: 0.1834\n",
      "Epoch 22/60\n",
      "172/172 [==============================] - 0s 687us/step - loss: 0.1834\n",
      "Epoch 23/60\n",
      "172/172 [==============================] - 0s 667us/step - loss: 0.1826\n",
      "Epoch 24/60\n",
      "172/172 [==============================] - 0s 677us/step - loss: 0.1811\n",
      "Epoch 25/60\n",
      "172/172 [==============================] - 0s 653us/step - loss: 0.1801\n",
      "Epoch 26/60\n",
      "172/172 [==============================] - 0s 645us/step - loss: 0.1795\n",
      "Epoch 27/60\n",
      "172/172 [==============================] - 0s 683us/step - loss: 0.1786\n",
      "Epoch 28/60\n",
      "172/172 [==============================] - 0s 672us/step - loss: 0.1779\n",
      "Epoch 29/60\n",
      "172/172 [==============================] - 0s 682us/step - loss: 0.1769\n",
      "Epoch 30/60\n",
      "172/172 [==============================] - 0s 672us/step - loss: 0.1755\n",
      "Epoch 31/60\n",
      "172/172 [==============================] - 0s 667us/step - loss: 0.1744\n",
      "Epoch 32/60\n",
      "172/172 [==============================] - 0s 671us/step - loss: 0.1752\n",
      "Epoch 33/60\n",
      "172/172 [==============================] - 0s 672us/step - loss: 0.1724\n",
      "Epoch 34/60\n",
      "172/172 [==============================] - 0s 659us/step - loss: 0.1722\n",
      "Epoch 35/60\n",
      "172/172 [==============================] - 0s 670us/step - loss: 0.1702\n",
      "Epoch 36/60\n",
      "172/172 [==============================] - 0s 663us/step - loss: 0.1685\n",
      "Epoch 37/60\n",
      "172/172 [==============================] - 0s 668us/step - loss: 0.1682\n",
      "Epoch 38/60\n",
      "172/172 [==============================] - 0s 655us/step - loss: 0.1661\n",
      "Epoch 39/60\n",
      "172/172 [==============================] - 0s 672us/step - loss: 0.1649\n",
      "Epoch 40/60\n",
      "172/172 [==============================] - 0s 661us/step - loss: 0.1652\n",
      "Epoch 41/60\n",
      "172/172 [==============================] - 0s 666us/step - loss: 0.1633\n",
      "Epoch 42/60\n",
      "172/172 [==============================] - 0s 666us/step - loss: 0.1630\n",
      "Epoch 43/60\n",
      "172/172 [==============================] - 0s 665us/step - loss: 0.1607\n",
      "Epoch 44/60\n",
      "172/172 [==============================] - 0s 667us/step - loss: 0.1603\n",
      "Epoch 45/60\n",
      "172/172 [==============================] - 0s 666us/step - loss: 0.1588\n",
      "Epoch 46/60\n",
      "172/172 [==============================] - 0s 672us/step - loss: 0.1573\n",
      "Epoch 47/60\n",
      "172/172 [==============================] - 0s 670us/step - loss: 0.1564\n",
      "Epoch 48/60\n",
      "172/172 [==============================] - 0s 655us/step - loss: 0.1560\n",
      "Epoch 49/60\n",
      "172/172 [==============================] - 0s 663us/step - loss: 0.1548\n",
      "Epoch 50/60\n",
      "172/172 [==============================] - 0s 653us/step - loss: 0.1537\n",
      "Epoch 51/60\n",
      "172/172 [==============================] - 0s 654us/step - loss: 0.1538\n",
      "Epoch 52/60\n",
      "172/172 [==============================] - 0s 656us/step - loss: 0.1516\n",
      "Epoch 53/60\n",
      "172/172 [==============================] - 0s 664us/step - loss: 0.1512\n",
      "Epoch 54/60\n",
      "172/172 [==============================] - 0s 667us/step - loss: 0.1502\n",
      "Epoch 55/60\n",
      "172/172 [==============================] - 0s 657us/step - loss: 0.1494\n",
      "Epoch 56/60\n",
      "172/172 [==============================] - 0s 661us/step - loss: 0.1485\n",
      "Epoch 57/60\n",
      "172/172 [==============================] - 0s 658us/step - loss: 0.1472\n",
      "Epoch 58/60\n",
      "172/172 [==============================] - 0s 658us/step - loss: 0.1473\n",
      "Epoch 59/60\n",
      "172/172 [==============================] - 0s 654us/step - loss: 0.1461\n",
      "Epoch 60/60\n",
      "172/172 [==============================] - 0s 675us/step - loss: 0.1444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f626c6ccf50>"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the NN\n",
    "print(item_train)\n",
    "tf.random.set_seed(1)\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt, loss=cost_fn)\n",
    "model.fit([user_train, item_train], y_train, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "f75f20b2-8412-4758-86aa-ad72f9a25670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 534us/step - loss: 0.2252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22523756325244904"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([user_test, item_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44266311-35dc-4b1c-b82c-811cae7d9d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
